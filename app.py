import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin


def crawl_and_save_blog_post(blog_id: str, post_num: str) -> str:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ IDì™€ ê²Œì‹œë¬¼ ë²ˆí˜¸ë¥¼ ë°›ì•„ HTMLê³¼ CSSë¥¼ ê²°í•©í•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        blog_id (str): í¬ë¡¤ë§í•  ë„¤ì´ë²„ ë¸”ë¡œê·¸ ID
        post_num (str): í¬ë¡¤ë§í•  ê²Œì‹œë¬¼ ë²ˆí˜¸

    Returns:
        str: ì €ì¥ëœ íŒŒì¼ì˜ ê²½ë¡œ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€
    """
    # 1. URL ìƒì„± ë° HTML ê°€ì ¸ì˜¤ê¸°
    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” ëª¨ë°”ì¼ ë²„ì „ í˜ì´ì§€ê°€ êµ¬ì¡°ê°€ ë” ê°„ë‹¨í•˜ê³  CSS ì²˜ë¦¬ê°€ ìš©ì´í•©ë‹ˆë‹¤.
    target_url = f"https://m.blog.naver.com/{blog_id}/{post_num}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(target_url, headers=headers)
        response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
    except requests.exceptions.RequestException as e:
        return f"âŒ ì˜¤ë¥˜: í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•˜ì„¸ìš”. ({e})"

    # 2. BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 3. í•„ìš”í•œ ìš”ì†Œë§Œ ì¶”ì¶œ (se-main-container ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§)
    main_container = soup.find(class_='se-main-container')
    if main_container:
        # ìƒˆë¡œìš´ HTML ë¬¸ì„œ ìƒì„±
        new_soup = BeautifulSoup('<!DOCTYPE html><html><head></head><body></body></html>', 'html.parser')
        new_soup.body.append(main_container.extract())
        soup = new_soup
    
    # 4. ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
    for unwanted in soup.find_all(class_=['blog_fe_feed', 'section_t1']):
        unwanted.decompose()
    
    # 5. ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„  ë° ìŠ¤íƒ€ì¼ ì ìš©
    # ëª¨ë“  ì´ë¯¸ì§€ì˜ srcë¥¼ ê³ í•´ìƒë„ë¡œ ë³€ê²½í•˜ê³  í•„ìš”í•œ ìŠ¤íƒ€ì¼ ì¶”ê°€
    for img in soup.find_all('img', class_='se-image-resource'):
        # ì €í•´ìƒë„ ë¸”ëŸ¬ ì´ë¯¸ì§€ë¥¼ ê³ í•´ìƒë„ë¡œ ë³€ê²½
        if img.get('src') and 'type=w80_blur' in img['src']:
            img['src'] = img['src'].replace('type=w80_blur', 'type=w800')
        
        # data-lazy-srcê°€ ìˆìœ¼ë©´ srcë¡œ ì„¤ì •
        if img.get('data-lazy-src'):
            img['src'] = img['data-lazy-src']
            
        # ì´ë¯¸ì§€ì— ê¸°ë³¸ ìŠ¤íƒ€ì¼ ì ìš©
        img['style'] = 'display: block !important; margin-left: auto !important; margin-right: auto !important; max-width: 100% !important; height: auto !important;'
    
    # ì´ë¯¸ì§€ ë§í¬ì—ë„ ìŠ¤íƒ€ì¼ ì ìš©
    for link in soup.find_all('a', class_='se-module-image-link'):
        link['style'] = 'display: block !important; text-align: center !important; margin-left: auto !important; margin-right: auto !important; width: fit-content !important;'

    # 6. CSS ì¶”ì¶œ ë° í†µí•©
    # ëª¨ë“  <link rel="stylesheet"> íƒœê·¸ì™€ <style> íƒœê·¸ ì°¾ê¸°
    css_tags = soup.find_all(['link', 'style'])

    combined_css = ""
    for tag in css_tags:
        if tag.name == 'style':
            # <style> íƒœê·¸ ì•ˆì˜ CSS ì½”ë“œ ì§ì ‘ ì¶”ê°€
            combined_css += tag.get_text() + "\n"
        elif tag.name == 'link' and tag.get('rel') == ['stylesheet'] and 'href' in tag.attrs:
            # <link> íƒœê·¸ì˜ href ì†ì„±ì—ì„œ CSS íŒŒì¼ URL ê°€ì ¸ì˜¤ê¸°
            css_url = urljoin(target_url, tag['href'])
            try:
                css_response = requests.get(css_url, headers=headers)
                if css_response.status_code == 200:
                    combined_css += css_response.text + "\n"
            except requests.exceptions.RequestException:
                # CSS íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                pass

        # ì›ë³¸ <link>ì™€ <style> íƒœê·¸ëŠ” ì‚­ì œí•˜ì—¬ ì¤‘ë³µ ë°©ì§€
        tag.decompose()

    # 7. ê¸°ë³¸ ìŠ¤íƒ€ì¼ CSS ì¶”ê°€
    try:
        with open("style.css", "r", encoding="utf-8") as f:
            base_css = f.read()
            # <style> íƒœê·¸ê°€ ìˆë‹¤ë©´ ì œê±°í•˜ê³  ë‚´ìš©ë§Œ ì¶”ì¶œ
            if base_css.strip().startswith('<style>') and base_css.strip().endswith('</style>'):
                base_css = base_css.strip()[7:-8]  # <style>ê³¼ </style> ì œê±°
    except FileNotFoundError:
        base_css = ""

    # 8. í†µí•©ëœ CSSë¥¼ ìƒˆë¡œìš´ <style> íƒœê·¸ë¡œ ë§Œë“¤ì–´ <head>ì— ì¶”ê°€
    # ê¸°ë³¸ ìŠ¤íƒ€ì¼ì„ ë¨¼ì € ì¶”ê°€í•˜ê³ , ê·¸ ë‹¤ìŒì— í˜ì´ì§€ì˜ CSS ì¶”ê°€
    final_css = base_css + "\n" + combined_css if base_css else combined_css
    
    if final_css:
        new_style_tag = soup.new_tag('style')
        new_style_tag.string = final_css

        # ë¶ˆí•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ì œê±° (ì„ íƒ ì‚¬í•­, ë¡œë”© ì†ë„ ê°œì„ )
        for script in soup.find_all('script'):
            script.decompose()

        soup.head.append(new_style_tag)

    # 9. ì™„ì„±ëœ HTMLì„ íŒŒì¼ë¡œ ì €ì¥
    output_filename = f"crawled_{blog_id}_{post_num}.html"
    with open(output_filename, "w", encoding="utf-8") as f:
        f.write(str(soup))

    return f"âœ… ì„±ê³µ! '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."


# --- Streamlit UI ë¶€ë¶„ ---
st.set_page_config(page_title="Naver Blog Crawler", page_icon="ğŸ“")

st.title("ğŸ“ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬")
st.markdown("ë¸”ë¡œê·¸ IDì™€ ê²Œì‹œë¬¼ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ë©´, í•´ë‹¹ í¬ìŠ¤íŠ¸ì˜ HTMLê³¼ CSSë¥¼ í•©ì³ ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")

st.info(
    "ğŸ’¡ **ì°¸ê³  URL**: `https://blog.naver.com/monkey_dream/223816008103`\n - **ë¸”ë¡œê·¸ ID**: `monkey_dream`\n - **ê²Œì‹œë¬¼ ë²ˆí˜¸**: `223816008103`",
    icon="â„¹ï¸")

# ì‚¬ìš©ì ì…ë ¥
blog_id_input = st.text_input("ë¸”ë¡œê·¸ ID (Blog ID)", placeholder="ì˜ˆ: monkey_dream")
post_num_input = st.text_input("ê²Œì‹œë¬¼ ë²ˆí˜¸ (Post Number)", placeholder="ì˜ˆ: 223816008103")


if st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘!"):
    st.markdown("---")
    if blog_id_input and post_num_input:
        with st.spinner("ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            result_message = crawl_and_save_blog_post(blog_id_input, post_num_input)

        if "ì„±ê³µ" in result_message:
            st.success(result_message)
            
            # ì›ë³¸ ë¸”ë¡œê·¸ ë§í¬ ìƒì„± ë° í‘œì‹œ
            original_url = f"https://blog.naver.com/{blog_id_input}/{post_num_input}"
            st.info(f"ğŸ”— **ì›ë³¸ ë¸”ë¡œê·¸ ë§í¬**: [{original_url}]({original_url})")
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ (ì„ íƒ ì‚¬í•­)
            output_filename = f"crawled_{blog_id_input}_{post_num_input}.html"
            with open(output_filename, "r", encoding="utf-8") as f:
                st.download_button(
                    label=f"ğŸ“„ {output_filename} ë‹¤ìš´ë¡œë“œ",
                    data=f.read(),
                    file_name=output_filename,
                    mime='text/html'
                )
        else:
            st.error(result_message)
    else:
        st.warning("ë¸”ë¡œê·¸ IDì™€ ê²Œì‹œë¬¼ ë²ˆí˜¸ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")